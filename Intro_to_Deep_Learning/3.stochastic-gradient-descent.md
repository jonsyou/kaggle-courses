[tutorial url](https://www.kaggle.com/ryanholbrook/stochastic-gradient-descent)

# Stochastic Gradient Descent

When first created, all of the network's weights are set randomly -- the network doesn't "know" anything yet. 

As with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). 

Training the network means adjusting its weights in such a way that it can transform the features into the target.

In addition to the training data, we need two more things:

- A ```"loss function"``` that measures how good the network's predictions are.
- An ```"optimizer"``` that can tell the network how to change its weights.

## The Loss Function

The loss function measures the disparity between the the target's true value and the value the model predicts.
A common loss function for regression problems is the **mean absolute error** or **MAE**. For each prediction ```y_pred```, MAE measures the disparity from the true target ```y_true``` by an absolute difference abs(```y_true``` - ```y_pred```).
The total MAE loss on a dataset is the mean of all these absolute differences.

![image](https://user-images.githubusercontent.com/74973306/104694745-fa19ea00-574e-11eb-84ad-7e91abc8c9d1.png)

*The mean absolute error is the average length between the fitted curve and the data points.*

Besides MAE, other loss functions you might see for regression problems are the **mean-squared error** (**MSE**) or the **Huber loss** (both available in Keras).

## The Optimizer - Stochastic Gradient Descent

The optimizer is an algorithm that adjusts the weights to minimize the loss.

Virtually all of the optimization algorithms used in deep learning belong to a family called **stochastic gradient descent**. They are iterative algorithms that train a network in steps. One **step** of training goes like this:

1. Sample some training data and run it through the network to make predictions.
2. Measure the loss between the predictions and the true values.
3. Finally, adjust the weights in a direction that makes the loss smaller.
Then just do this over and over until the loss is as small as you like (or until it won't decrease any further.)

![image](https://i.imgur.com/rFI1tIk.gif)

*Training a neural network with Stochastic Gradient Descent.*

Each iteration's sample of training data is called a **minibatch** (or often just "batch"), while a complete round of the training data is called an **epoch**. The number of epochs you train for is how many times the network will see each training example.

## Learning Rate and Batch Size

Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.

The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds.

Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. **Adam** is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is "self tuning", in a sense). 

## Adding the Loss and Optimizer

After defining a model, you can add a loss function and optimizer with the model's compile method:

```python
model.compile(
    optimizer="adam",
    loss="mae",
)
```

## Example - Red Wine Quality

This dataset consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste-tests.


| fixed acidity | volatile acidity | citric acid | residual sugar | chlorides | free sulfur dioxide | total sulfur dioxide | density | pH      | sulphates | alcohol | quality |   |
|---------------|------------------|-------------|----------------|-----------|---------------------|----------------------|---------|---------|-----------|---------|---------|---|
| 1109          | 10.8             | 0.470       | 0.43           | 2.10      | 0.171               | 27.0                 | 66.0    | 0.99820 | 3.17      | 0.76    | 10.8    | 6 |
| 1032          | 8.1              | 0.820       | 0.00           | 4.10      | 0.095               | 5.0                  | 14.0    | 0.99854 | 3.36      | 0.53    | 9.6     | 5 |
| 1002          | 9.1              | 0.290       | 0.33           | 2.05      | 0.063               | 13.0                 | 27.0    | 0.99516 | 3.26      | 0.84    | 11.7    | 7 |
| 487           | 10.2             | 0.645       | 0.36           | 1.80      | 0.053               | 5.0                  | 14.0    | 0.99820 | 3.17      | 0.42    | 10.0    | 6 |

- setup
```python
import pandas as pd
from IPython.display import display

red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')

# Create training and validation splits
df_train = red_wine.sample(frac=0.7, random_state=0)
df_valid = red_wine.drop(df_train.index)
display(df_train.head(4))

# Scale to [0, 1]
max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)
df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)

# Split features and target
X_train = df_train.drop('quality', axis=1)
X_valid = df_valid.drop('quality', axis=1)
y_train = df_train['quality']
y_valid = df_valid['quality']

print(X_train.shape)

[output]
'''
(1119, 11)
'''
```
Neural networks tend to perform best when their inputs are on a common scale.

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
```

After defining the model, we compile in the optimizer and loss function.

```python
model.compile(
    optimizer='adam',
    loss='mae',
)
```

```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=10,
)

[output]
'''
Epoch 1/10
5/5 [==============================] - 0s 43ms/step - loss: 0.2914 - val_loss: 0.1361
Epoch 2/10
5/5 [==============================] - 0s 13ms/step - loss: 0.1413 - val_loss: 0.1298
Epoch 3/10
5/5 [==============================] - 0s 12ms/step - loss: 0.1276 - val_loss: 0.1188
Epoch 4/10
5/5 [==============================] - 0s 13ms/step - loss: 0.1194 - val_loss: 0.1166
Epoch 5/10
5/5 [==============================] - 0s 13ms/step - loss: 0.1150 - val_loss: 0.1070
Epoch 6/10
5/5 [==============================] - 0s 12ms/step - loss: 0.1094 - val_loss: 0.1070
Epoch 7/10
5/5 [==============================] - 0s 12ms/step - loss: 0.1076 - val_loss: 0.1074
Epoch 8/10
5/5 [==============================] - 0s 13ms/step - loss: 0.1075 - val_loss: 0.1057
Epoch 9/10
5/5 [==============================] - 0s 14ms/step - loss: 0.1104 - val_loss: 0.1149
Epoch 10/10
5/5 [==============================] - 0s 12ms/step - loss: 0.1058 - val_loss: 0.1035
'''
```

```python
import pandas as pd

# convert the training history to a dataframe
history_df = pd.DataFrame(history.history)
# use Pandas native plot method
history_df['loss'].plot();
```

output

![image](https://user-images.githubusercontent.com/74973306/104696965-5fbba580-5752-11eb-982f-34944c95a769.png)

---

## Exercise

### Setup
```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.model_selection import train_test_split

fuel = pd.read_csv('../input/dl-course-data/fuel.csv')

X = fuel.copy()
# Remove target
y = X.pop('FE')

preprocessor = make_column_transformer(
    (StandardScaler(),
     make_column_selector(dtype_include=np.number)),
    (OneHotEncoder(sparse=False),
     make_column_selector(dtype_include=object)),
)

X = preprocessor.fit_transform(X)
y = np.log(y) # log transform target instead of standardizing

input_shape = [X.shape[1]]
print("Input shape: {}".format(input_shape))

[output]
'''
Input shape: [50]

'''
```

### ```keras.Sequential```
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=input_shape),
    layers.Dense(128, activation='relu'),    
    layers.Dense(64, activation='relu'),
    layers.Dense(1),
])
```
### Add Loss and Optimizer
```python
model.compile(
    optimizer='adam',
    loss='mae'
)
```

### Train Model
```python
history = model.fit(
    X, y,
    batch_size=128,
    epochs=200
)

[output]
'''
Epoch 1/200
9/9 [==============================] - 0s 2ms/step - loss: 2.9679
Epoch 2/200
9/9 [==============================] - 0s 2ms/step - loss: 1.0494
Epoch 3/200
9/9 [==============================] - 0s 2ms/step - loss: 0.6467
Epoch 4/200
9/9 [==============================] - 0s 2ms/step - loss: 0.4172
Epoch 5/200
9/9 [==============================] - 0s 2ms/step - loss: 0.2948
Epoch 6/200
9/9 [==============================] - 0s 2ms/step - loss: 0.2308
                                .
                                .
                                .
                                .
Epoch 194/200
9/9 [==============================] - 0s 2ms/step - loss: 0.0254
Epoch 195/200
9/9 [==============================] - 0s 2ms/step - loss: 0.0255
Epoch 196/200
9/9 [==============================] - 0s 2ms/step - loss: 0.0255
Epoch 197/200
9/9 [==============================] - 0s 2ms/step - loss: 0.0248
Epoch 198/200
9/9 [==============================] - 0s 2ms/step - loss: 0.0278
Epoch 199/200
9/9 [==============================] - 0s 2ms/step - loss: 0.0296
Epoch 200/200
9/9 [==============================] - 0s 2ms/step - loss: 0.0332
'''
```

```python
import pandas as pd

history_df = pd.DataFrame(history.history)
# Start the plot at epoch 5. You can change this to get a different view.
history_df.loc[5:, ['loss']].plot();
```
output

![image](https://user-images.githubusercontent.com/74973306/104698133-38fe6e80-5754-11eb-94a8-2acd31b393b6.png)

### Evaluate Training

With the learning rate and the batch size, you have some control over:

- How long it takes to train a model
- How noisy the learning curves are
- How small the loss becomes

### Learning Rate and Batch Size

Smaller batch sizes gave noisier weight updates and loss curves. This is because each batch is a small sample of data and smaller samples tend to give noisier estimates. Smaller batches can have an "averaging" effect though which can be beneficial.

Smaller learning rates make the updates smaller and the training takes longer to converge. Large learning rates can speed up training, but don't "settle in" to a minimum as well. When the learning rate is too large, the training can fail completely. 
