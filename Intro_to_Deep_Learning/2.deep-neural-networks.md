[tutorial url](https://www.kaggle.com/ryanholbrook/deep-neural-networks)

# Deep Neural Networks

The key idea here is *modularity*, building up a complex network from simpler functional units. 

## Layers

Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.

![image](https://user-images.githubusercontent.com/74973306/104691730-24b57400-574a-11eb-8a08-fe2a43feb2d8.png)

*A dense layer of two linear units receiving two inputs and a bias.*

Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways.

## The Activation Function

It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself.
What we need is something nonlinear. What we need are activation functions.

![image](https://user-images.githubusercontent.com/74973306/104692042-9ee5f880-574a-11eb-8371-ed831e588a03.png)

*Without activation functions, neural networks can only learn linear relationships.*  
*In order to fit curves, we'll need to use activation functions.*

An *activation function* is simply some function we apply to each of a layer's outputs (its *activations*). The most common is the rectifier function  ```max(0,x)``` .

![image](https://user-images.githubusercontent.com/74973306/104692190-e7051b00-574a-11eb-9820-60aec6db1d7a.png)

The rectifier function has a graph that's a line with the negative part "rectified" to zero. Applying the function to the outputs of a neuron **will put a bend in the data**, **moving us away from simple lines**.

When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. (For this reason, it's common to call the rectifier function the "ReLU function".) Applying a **ReLU** activation to a linear unit means the output becomes ```max(0, w * x + b)```, which we might draw in a diagram like:

![image](https://user-images.githubusercontent.com/74973306/104692419-4cf1a280-574b-11eb-9640-b6499f45cabe.png)

*A rectified linear unit.*

## Stacking Dense Layers

![image](https://user-images.githubusercontent.com/74973306/104692468-6561bd00-574b-11eb-84fb-99e2fc8ba4cd.png)

*A stack of dense layers makes a "fully-connected" network.*

Now, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.

## Building Sequential Models

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # the hidden ReLU layers
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # the linear output layer 
    layers.Dense(units=1),
])
```

### Activation Layers
Sometimes though you'll want to put some other layer between the Dense layer and its activation function. 
In this case, we can define the activation in its own Activation layer, like so:

```python
layers.Dense(units=8),
layers.Activation('relu')
```

### Optional: Alternatives to ReLU
Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. 

```python
def activation_plot(activation):
    activation_layer = layers.Activation(activation)

    x = tf.linspace(-3.0, 3.0, 100)
    y = activation_layer(x) # once created, a layer is callable just like a function

    plt.figure(dpi=100)
    plt.plot(x, y)
    plt.xlim(-3, 3)
    plt.xlabel("Input")
    plt.ylabel("Output")
    plt.title(activation)
    plt.show()
    
activation_plot('relu')
activation_plot('elu')
activation_plot('selu')
activation_plot('swish')
```

output

![image](https://user-images.githubusercontent.com/74973306/104693851-ae1a7580-574d-11eb-80ec-3547cff9e765.png)

![image](https://user-images.githubusercontent.com/74973306/104693879-b7a3dd80-574d-11eb-94b0-605d1bcefd7e.png)

![image](https://user-images.githubusercontent.com/74973306/104693898-becaeb80-574d-11eb-9904-d0c348c2680b.png)

![image](https://user-images.githubusercontent.com/74973306/104693919-c7232680-574d-11eb-88ba-ae09356a2497.png)
