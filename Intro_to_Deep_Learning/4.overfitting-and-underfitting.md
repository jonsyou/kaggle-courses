[tutorial url](https://www.kaggle.com/ryanholbrook/overfitting-and-underfitting)

# Overfitting and Underfitting

## Interpreting the Learning Curves

The information in the training data consists of two things.
- signal : the part that generalizes, the part that can help our model make predictions from new data
- noise : all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. 

We train a model by choosing weights or parameters that minimize the loss on a training set. 
To accurately evaluate the performance of your model, you need to evaluate the new dataset, the validation data.

These plots we call the **learning curves**. To train deep learning models effectively, we need to be able to interpret them.

![image](https://user-images.githubusercontent.com/74973306/104865883-8a8b4100-5980-11eb-85ee-5250d9e41173.png)

*The validation loss gives an estimate of the expected error on unseen data.*

When a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.

After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.

![image](https://user-images.githubusercontent.com/74973306/104866063-fc638a80-5980-11eb-848e-85cf13037a41.png)

*Underfitting and overfitting.*

This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. 
- **Underfitting** : The training set is when the loss is not as low as it could be because the model hasn't learned enough signal.
- **Overfitting** : The training set is when the loss is not as low as it could be because the model learned too much noise.

## Capacity

A model's **capacity** refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.

Two ways to increase the capacity of a network
- more units to existing layers
- adding more layers

Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.

```python
model = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])

wider = keras.Sequential([
    layers.Dense(32, activation='relu'),
    layers.Dense(1),
])

deeper = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])
```

## Early Stopping

When a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore.Interrupting the training this way is called **early stopping**.

![image](https://user-images.githubusercontent.com/74973306/104872084-36885880-5990-11eb-8443-3c8d45133670.png)

*We keep the model where the validation loss is at a minimum.*

Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won't continue to learn noise and overfit the data.

Besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough.

## Adding Early Stopping

A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch.

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)
```
These parameters say: "If there hasn't been at least an improvement of 0.001 in the validation loss over the previous 20 epochs, then stop the training and keep the best model you found." It can sometimes be hard to tell if the validation loss is rising due to overfitting or just due to random batch variation. The parameters allow us to set some allowances around when to stop.

## Example - Train a Model with Early Stopping

```python
import pandas as pd
from IPython.display import display

red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')

# Create training and validation splits
df_train = red_wine.sample(frac=0.7, random_state=0)
df_valid = red_wine.drop(df_train.index)
display(df_train.head(4))

# Scale to [0, 1]
max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)
df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)

# Split features and target
X_train = df_train.drop('quality', axis=1)
X_valid = df_valid.drop('quality', axis=1)
y_train = df_train['quality']
y_valid = df_valid['quality']
```

	fixed acidity	volatile acidity	citric acid	residual sugar	chlorides	free sulfur dioxide	total sulfur dioxide	density	pH	sulphates	alcohol	quality
1109	10.8	0.470	0.43	2.10	0.171	27.0	66.0	0.99820	3.17	0.76	10.8	6
1032	8.1	0.820	0.00	4.10	0.095	5.0	14.0	0.99854	3.36	0.53	9.6	5
1002	9.1	0.290	0.33	2.05	0.063	13.0	27.0	0.99516	3.26	0.84	11.7	7
487	10.2	0.645	0.36	1.80	0.053	5.0	14.0	0.99820	3.17	0.42	10.0	6

```python
from tensorflow import keras
from tensorflow.keras import layers, callbacks

early_stopping = callbacks.EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
model.compile(
    optimizer='adam',
    loss='mae',
)
```
After defining the callback, add it as an argument in ```fit```.

```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500,
    callbacks=[early_stopping], # put your callbacks in a list
    verbose=0,  # turn off training log
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))

[output]
'''
Minimum validation loss: 0.09167633205652237
'''
```

![image](https://user-images.githubusercontent.com/74973306/104872837-486afb00-5992-11eb-93df-adf96ab527c3.png)
