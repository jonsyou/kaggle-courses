[tutorial url](https://www.kaggle.com/ryanholbrook/overfitting-and-underfitting)

# Overfitting and Underfitting

## Interpreting the Learning Curves

The information in the training data consists of two things.
- signal : the part that generalizes, the part that can help our model make predictions from new data
- noise : all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. 

We train a model by choosing weights or parameters that minimize the loss on a training set. 
To accurately evaluate the performance of your model, you need to evaluate the new dataset, the validation data.

These plots we call the **learning curves**. To train deep learning models effectively, we need to be able to interpret them.

![image](https://user-images.githubusercontent.com/74973306/104865883-8a8b4100-5980-11eb-85ee-5250d9e41173.png)

*The validation loss gives an estimate of the expected error on unseen data.*

When a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.

After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.

![image](https://user-images.githubusercontent.com/74973306/104866063-fc638a80-5980-11eb-848e-85cf13037a41.png)

*Underfitting and overfitting.*

This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. 
- **Underfitting** : The training set is when the loss is not as low as it could be because the model hasn't learned enough signal.
- **Overfitting** : The training set is when the loss is not as low as it could be because the model learned too much noise.

## Capacity
