[tutorial url](https://www.kaggle.com/ryanholbrook/overfitting-and-underfitting)

# Overfitting and Underfitting

## Interpreting the Learning Curves

The information in the training data consists of two things.
- signal : the part that generalizes, the part that can help our model make predictions from new data
- noise : all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. 

We train a model by choosing weights or parameters that minimize the loss on a training set. 
To accurately evaluate the performance of your model, you need to evaluate the new dataset, the validation data.

These plots we call the **learning curves**. To train deep learning models effectively, we need to be able to interpret them.

![image](https://user-images.githubusercontent.com/74973306/104865883-8a8b4100-5980-11eb-85ee-5250d9e41173.png)

*The validation loss gives an estimate of the expected error on unseen data.*

When a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.

After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.

![image](https://user-images.githubusercontent.com/74973306/104866063-fc638a80-5980-11eb-848e-85cf13037a41.png)

*Underfitting and overfitting.*

This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. 
- **Underfitting** : The training set is when the loss is not as low as it could be because the model hasn't learned enough signal.
- **Overfitting** : The training set is when the loss is not as low as it could be because the model learned too much noise.

## Capacity

A model's **capacity** refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.

Two ways to increase the capacity of a network
- more units to existing layers
- adding more layers

Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.

```python
model = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])

wider = keras.Sequential([
    layers.Dense(32, activation='relu'),
    layers.Dense(1),
])

deeper = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])
```

## Early Stopping

When a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore.Interrupting the training this way is called **early stopping**.

![image](https://user-images.githubusercontent.com/74973306/104872084-36885880-5990-11eb-8443-3c8d45133670.png)

*We keep the model where the validation loss is at a minimum.*

Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won't continue to learn noise and overfit the data.

Besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough.

## Adding Early Stopping

A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch.

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)
```
These parameters say: "If there hasn't been at least an improvement of 0.001 in the validation loss over the previous 20 epochs, then stop the training and keep the best model you found." It can sometimes be hard to tell if the validation loss is rising due to overfitting or just due to random batch variation. The parameters allow us to set some allowances around when to stop.

## Example - Train a Model with Early Stopping

```python
import pandas as pd
from IPython.display import display

red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')

# Create training and validation splits
df_train = red_wine.sample(frac=0.7, random_state=0)
df_valid = red_wine.drop(df_train.index)
display(df_train.head(4))

# Scale to [0, 1]
max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)
df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)

# Split features and target
X_train = df_train.drop('quality', axis=1)
X_valid = df_valid.drop('quality', axis=1)
y_train = df_train['quality']
y_valid = df_valid['quality']
```

	fixed acidity	volatile acidity	citric acid	residual sugar	chlorides	free sulfur dioxide	total sulfur dioxide	density	pH	sulphates	alcohol	quality
1109	10.8	0.470	0.43	2.10	0.171	27.0	66.0	0.99820	3.17	0.76	10.8	6
1032	8.1	0.820	0.00	4.10	0.095	5.0	14.0	0.99854	3.36	0.53	9.6	5
1002	9.1	0.290	0.33	2.05	0.063	13.0	27.0	0.99516	3.26	0.84	11.7	7
487	10.2	0.645	0.36	1.80	0.053	5.0	14.0	0.99820	3.17	0.42	10.0	6

```python
from tensorflow import keras
from tensorflow.keras import layers, callbacks

early_stopping = callbacks.EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
model.compile(
    optimizer='adam',
    loss='mae',
)
```
After defining the callback, add it as an argument in ```fit```.

```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500,
    callbacks=[early_stopping], # put your callbacks in a list
    verbose=0,  # turn off training log
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))

[output]
'''
Minimum validation loss: 0.09167633205652237
'''
```

![image](https://user-images.githubusercontent.com/74973306/104872837-486afb00-5992-11eb-93df-adf96ab527c3.png)

---

## Excercise

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.model_selection import GroupShuffleSplit

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import callbacks

spotify = pd.read_csv('../input/dl-course-data/spotify.csv')

X = spotify.copy().dropna()
y = X.pop('track_popularity')
artists = X['track_artist']

features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',
                'speechiness', 'acousticness', 'instrumentalness',
                'liveness', 'valence', 'tempo', 'duration_ms']
features_cat = ['playlist_genre']

preprocessor = make_column_transformer(
    (StandardScaler(), features_num),
    (OneHotEncoder(), features_cat),
)

# We'll do a "grouped" split to keep all of an artist's songs in one
# split or the other. This is to help prevent signal leakage.
def group_split(X, y, group, train_size=0.75):
    splitter = GroupShuffleSplit(train_size=train_size)
    train, test = next(splitter.split(X, y, groups=group))
    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])

X_train, X_valid, y_train, y_valid = group_split(X, y, artists)

X_train = preprocessor.fit_transform(X_train)
X_valid = preprocessor.transform(X_valid)
y_train = y_train / 100 # popularity is on a scale 0-100, so this rescales to 0-1.
y_valid = y_valid / 100

input_shape = [X_train.shape[1]]
print("Input shape: {}".format(input_shape))

[output]
'''
Input shape: [18]
'''
```

```python
model = keras.Sequential([
    layers.Dense(1, input_shape=input_shape),
])
model.compile(
    optimizer='adam',
    loss='mae',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=50,
    verbose=0, # suppress output since we'll plot the curves
)
history_df = pd.DataFrame(history.history)
history_df.loc[0:, ['loss', 'val_loss']].plot()
print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()));

[output]
'''
Minimum Validation Loss: 0.1929
'''
```

![image](https://user-images.githubusercontent.com/74973306/104873042-e52d9880-5992-11eb-9e98-21b811ac9512.png)

### 1) Evaluate Baseline

```python
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=input_shape),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])
model.compile(
    optimizer='adam',
    loss='mae',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=50,
)
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()));

[output]
'''
Epoch 1/50
49/49 [==============================] - 0s 4ms/step - loss: 0.2622 - val_loss: 0.2083
Epoch 2/50
49/49 [==============================] - 0s 2ms/step - loss: 0.2061 - val_loss: 0.1996
Epoch 3/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1991 - val_loss: 0.1984
                                             .
					     .
					     .
Epoch 48/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1700 - val_loss: 0.2010
Epoch 49/50
49/49 [==============================] - 0s 2ms/step - loss: 0.1692 - val_loss: 0.2005
Epoch 50/50
49/49 [==============================] - 0s 2ms/step - loss: 0.1686 - val_loss: 0.2001
Minimum Validation Loss: 0.1929
'''
```

![image](https://user-images.githubusercontent.com/74973306/104873243-69801b80-5993-11eb-9943-e5ccb5adf313.png)

### 2) Add Capacity

Now the validation loss begins to rise very early, while the training loss continues to decrease. This indicates that the network has begun to overfit.

### 3) Define Early Stopping Callback

```python
from tensorflow.keras import callbacks

# YOUR CODE HERE: define an early stopping callback
early_stopping = callbacks.EarlyStopping(
    patience=5,
    min_delta=0.001,
    restore_best_weights=True,
)
```

```python
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=input_shape),
    layers.Dense(64, activation='relu'),    
    layers.Dense(1)
])
model.compile(
    optimizer='adam',
    loss='mae',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=50,
    callbacks=[early_stopping]
)
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()));

[output]
'''
Epoch 1/50
49/49 [==============================] - 0s 7ms/step - loss: 0.2308 - val_loss: 0.2030
Epoch 2/50
49/49 [==============================] - 0s 5ms/step - loss: 0.2026 - val_loss: 0.1990
Epoch 3/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1974 - val_loss: 0.1961
                                                .
						.
						.
Epoch 11/50
49/49 [==============================] - 0s 2ms/step - loss: 0.1858 - val_loss: 0.1936
Epoch 12/50
49/49 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 0.1939
Epoch 13/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1839 - val_loss: 0.1947
Minimum Validation Loss: 0.1931						
'''
```

![image](https://user-images.githubusercontent.com/74973306/104873509-06db4f80-5994-11eb-957c-ca9d88f1dda3.png)

### 4) Train and Interpret

The early stopping callback did stop the training once the network began overfitting. Moreover, by including ```restore_best_weights``` we still get to keep the model where validation loss was lowest.
